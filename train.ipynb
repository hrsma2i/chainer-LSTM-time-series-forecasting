{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from itertools import product, chain, islice\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "import chainer\n",
    "from chainer import Variable, training, optimizers, reporter\n",
    "from chainer.training import extensions, util\n",
    "from chainer.iterators import SerialIterator\n",
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "\n",
    "from model import RNN\n",
    "from data_process import Processer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LossSumMSEOverTime(L.Classifier):\n",
    "    def __init__(self, predictor):\n",
    "        super(LossSumMSEOverTime, self).__init__(predictor, lossfun=F.mean_squared_error)\n",
    "    \n",
    "    def __call__(self, X_STF, y_STF):\n",
    "        \"\"\"\n",
    "        # Param\n",
    "        - X_STF (Variable: (S, T, F))\n",
    "        - y_STF (Variable: (S, T, F))\n",
    "        S: samples\n",
    "        T: time_steps\n",
    "        F: features\n",
    "        \n",
    "        # Return\n",
    "        - loss (Variable: (1, ))\n",
    "        \"\"\"\n",
    "        X_TSF = X_STF.transpose(1,0,2)\n",
    "        y_TSF = y_STF.transpose(1,0,2)\n",
    "        seq_len  = X_TSF.shape[0]\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            pred = self.predictor(X_TSF[t])\n",
    "            obs  = y_TSF[t]\n",
    "            loss += self.lossfun(pred, obs)\n",
    "        loss /= seq_len\n",
    "        \n",
    "        reporter.report({'loss': loss}, self)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UpdaterRNN(training.StandardUpdater):\n",
    "    def __init__(self, itr_train, optimizer, device=-1):\n",
    "        super(UpdaterRNN, self).__init__(itr_train, optimizer, device=device)\n",
    "        \n",
    "    # overrided\n",
    "    def update_core(self):\n",
    "        itr_train = self.get_iterator('main')\n",
    "        optimizer = self.get_optimizer('main')\n",
    "        \n",
    "        batch = itr_train.__next__()\n",
    "        X_STF, y_STF = chainer.dataset.concat_examples(batch, self.device)\n",
    "        \n",
    "        optimizer.target.zerograds()\n",
    "        optimizer.target.predictor.reset_state()\n",
    "        loss = optimizer.target(Variable(X_STF), Variable(y_STF))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO 連続で勾配がeps以上上昇したら\n",
    "class ExploasionStoppingTrigger(object):\n",
    "\n",
    "    def __init__(self, max_epoch, key, stop_condition=None, \n",
    "                 eps=10, trigger=(1, 'epoch')):\n",
    "        self.max_epoch = max_epoch\n",
    "        self.eps = eps\n",
    "        self._key = key\n",
    "        self._current_value = None\n",
    "        self._interval_trigger = util.get_trigger(trigger)\n",
    "        self._init_summary()\n",
    "        self.stop_condition = stop_condition or self._stop_condition\n",
    "\n",
    "    def __call__(self, trainer):\n",
    "        \"\"\"Decides whether the extension should be called on this iteration.\n",
    "        Args:\n",
    "            trainer (~chainer.training.Trainer): Trainer object that this\n",
    "                trigger is associated with. The ``observation`` of this trainer\n",
    "                is used to determine if the trigger should fire.\n",
    "        Returns:\n",
    "            bool: ``True`` if the corresponding extension should be invoked in\n",
    "                this iteration.\n",
    "        \"\"\"\n",
    "\n",
    "        epoch_detail = trainer.updater.epoch_detail\n",
    "        if self.max_epoch <= epoch_detail:\n",
    "            print('Reached to max_epoch.')\n",
    "            return True\n",
    "\n",
    "        observation = trainer.observation\n",
    "        summary = self._summary\n",
    "        key = self._key\n",
    "        if key in observation:\n",
    "            summary.add({key: observation[key]})\n",
    "\n",
    "        if not self._interval_trigger(trainer):\n",
    "            return False\n",
    "\n",
    "        stats = summary.compute_mean()\n",
    "        value = float(stats[key])  # copy to CPU\n",
    "        self._init_summary()\n",
    "\n",
    "        if self._current_value is None:\n",
    "            self._current_value = value\n",
    "            return False\n",
    "        else:\n",
    "            if self.stop_condition(self._current_value, value):\n",
    "                # print('Previous value {}, Current value {}'\n",
    "                #       .format(self._current_value, value))\n",
    "                print('Invoke ExploasionStoppingTrigger...')\n",
    "                self._current_value = value\n",
    "                return True\n",
    "            else:\n",
    "                self._current_value = value\n",
    "                return False\n",
    "\n",
    "    def _init_summary(self):\n",
    "        self._summary = reporter.DictSummary()\n",
    "\n",
    "    def _stop_condition(self, current_value, new_value):\n",
    "        return new_value - current_value > self.eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hp2name(hp):\n",
    "    d = {\n",
    "        'u':hp['units'],\n",
    "        'opt':hp['optimizer'].__class__.__name__\n",
    "    }\n",
    "    name = '_'.join([k+str(v) for k, v in d.items()])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hp2json(hp):\n",
    "    hp_json = {\n",
    "        'units':hp['units'],\n",
    "        'optimizer':hp['optimizer'].__class__.__name__\n",
    "    }\n",
    "    return hp_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(datasets, hp, out, n_epoch):\n",
    "    \"\"\"\n",
    "    dump the given hyperparameters hp, and\n",
    "    train a model with the hyperparameters\n",
    "    \n",
    "    # Param\n",
    "    - datasets (tuple): ds_train, ds_val\n",
    "    - hp (dict): hyperparameters\n",
    "    - out (str): the path where \"log\" and \"hyperparameters\"\n",
    "                will be dumped\n",
    "    - n_epoch: up to which train the model\n",
    "    \"\"\"\n",
    "    # dump hyperparameters\n",
    "    if not os.path.exists(out):\n",
    "        os.mkdir(out)\n",
    "    path_json = os.path.join(out, 'hyperparameters.json')\n",
    "    hp_json = hp2json(hp)\n",
    "    json.dump(hp_json, open(path_json, 'w'))\n",
    "    \n",
    "    \n",
    "    # training\n",
    "    units = hp['units']\n",
    "    optimizer = hp['optimizer']\n",
    "    \n",
    "    model = LossSumMSEOverTime(RNN(units))\n",
    "    \n",
    "    optimizer.setup(model)\n",
    "    \n",
    "    \n",
    "    ds_train, ds_val = datasets\n",
    "    itr_train = SerialIterator(ds_train, batch_size=1, shuffle=False)\n",
    "    itr_val   = SerialIterator(ds_val  , batch_size=1, shuffle=False, repeat=False)\n",
    "    \n",
    "    updater = UpdaterRNN(itr_train, optimizer)\n",
    "    \n",
    "    eval_model = model.copy()\n",
    "    eval_rnn = eval_model.predictor\n",
    "    expl_stop = ExploasionStoppingTrigger(n_epoch, \n",
    "                        key='validation/main/loss', eps=10)\n",
    "    trainer = training.Trainer(updater, \n",
    "                               stop_trigger=expl_stop, out=out)\n",
    "    trainer.extend(extensions.Evaluator(\n",
    "                itr_val, eval_model, device=-1,\n",
    "                eval_hook=lambda _: eval_rnn.reset_state()))\n",
    "    \n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.snapshot_object(model.predictor, \n",
    "                                               filename='model_epoch-{.updater.epoch}'))\n",
    "    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
    "                                        x_key='epoch', file_name='loss.png'))\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "                    ['epoch','main/loss','validation/main/loss']\n",
    "                ))\n",
    "    \n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tune(root, datasets, n_sample=10, n_epoch=5):\n",
    "    # search space\n",
    "    max_n_layer = 5\n",
    "    max_n_unit  = 5\n",
    "    opts = [\n",
    "        optimizers.SGD(),\n",
    "        optimizers.Adam(),\n",
    "        optimizers.RMSprop(),\n",
    "        optimizers.AdaDelta(),\n",
    "        optimizers.NesterovAG(),\n",
    "        optimizers.MomentumSGD(),\n",
    "    ]\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i in range(n_sample):\n",
    "        n_layer = np.random.randint(1, max_n_layer+1)\n",
    "        units = tuple(\n",
    "                    np.random.randint(1, max_n_unit+1)\n",
    "                    for l in range(n_layer)\n",
    "                )\n",
    "        optimizer = np.random.choice(opts)\n",
    "        \n",
    "        hp = {\n",
    "            'units': units,\n",
    "            'optimizer': optimizer,\n",
    "        }\n",
    "        path_hp = os.path.join(root, hp2name(hp))\n",
    "        \n",
    "        hp_json = hp2json(hp)\n",
    "        result.append(hp_json)\n",
    "        \n",
    "        display('sample{}'.format(i), pd.Series(hp_json))\n",
    "        print('path_hp', path_hp)\n",
    "        print()\n",
    "        \n",
    "        # training\n",
    "        train(datasets=datasets, hp=hp, \n",
    "              out=path_hp, n_epoch=n_epoch)\n",
    "        \n",
    "        print(''.join(['-' * 60]))\n",
    "\n",
    "    df = pd.DataFrame(result)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "def loop_prc(root, series):\n",
    "    \"\"\"\n",
    "    # Param\n",
    "    - root   (str): the path where results will be saved\n",
    "    - series (ndarray: (T, F)):\n",
    "    \"\"\"\n",
    "    def routin(name_prc, prscr, root=root, series=series):\n",
    "        path_prc = os.path.join(root, name_prc)\n",
    "        datasets = prcsr.get_datasets(series)\n",
    "        tune(root=path_prc, datasets=datasets)\n",
    "        \n",
    "    name_prc = 'not_log'\n",
    "    prcsr = Processer(log=False)\n",
    "    routin(name_prc, prcsr)\n",
    "        \n",
    "    name_prc = 'not_diff'\n",
    "    prcsr = Processer(diff=False)\n",
    "    routin(name_prc, prcsr)\n",
    "    \n",
    "    name_prc = 'not_diff'\n",
    "    prcsr = Processer(diff=False)\n",
    "    routin(name_prc, prcsr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loop_seq(root, prcsr=Processer()):\n",
    "    seqs = [\n",
    "        'airline',\n",
    "        'car',\n",
    "        'companyx',\n",
    "        'house',\n",
    "        'winnebago'\n",
    "    ]\n",
    "    \n",
    "    for name_seq in seqs:\n",
    "\n",
    "        series = pd.read_csv('data/{}_train.csv'.format(name_seq)\n",
    "                             , header=None).values.flatten()\n",
    "        if series.ndim == 1:\n",
    "            print('features = 1')\n",
    "            series = series.reshape(-1, 1)\n",
    "\n",
    "        path_seq = os.path.join(root, name_seq)\n",
    "        print(path_seq)\n",
    "        if not os.path.exists(path_seq):\n",
    "            os.mkdir(path_seq)\n",
    "\n",
    "        datasets = prcsr.get_datasets(series)\n",
    "\n",
    "        tune(root=path_seq, datasets=datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    root = 'result/test_seq_loop'\n",
    "    prcsr = Processer()\n",
    "    loop_seq(root=root, prcsr=prcsr)\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train a model\n",
    "if __name__==\"__main__\":\n",
    "    #prcsr = Processer(log_trnsfmr=log_trnsfmr, diff=diff, \n",
    "    #                  sclr=sclr, ysclr=ysclr)\n",
    "    lr = 0.01\n",
    "    prcsr = Processer()\n",
    "\n",
    "    series = pd.read_csv('data/airline_train.csv', header=None).values.flatten()\n",
    "    if series.ndim == 1:\n",
    "        print('ndim = 1')\n",
    "        series = series.reshape(-1, 1)\n",
    "\n",
    "    hp = {\n",
    "        'units':(3, 7, 3),\n",
    "        'optimizer':optimizers.Adam(alpha=lr)\n",
    "    }    \n",
    "    print(hp['optimizer'].alpha)\n",
    "    \n",
    "\n",
    "    root = 'result/test/adam{}'.format(lr)\n",
    "\n",
    "    datasets = prcsr.get_datasets(series)\n",
    "    \n",
    "    # training\n",
    "    #train(datasets, hp, out=root, n_epoch=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
